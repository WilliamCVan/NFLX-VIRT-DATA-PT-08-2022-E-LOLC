{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGA5-htN6oEN"
      },
      "outputs": [],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example: 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIUo9MYO6vlj"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "# Recall that the default shuffle partitions is 200.  We want to bring that down to a reasonable size for both our data and our Spark cluster\n",
        "# 4 is reasonable for a free Colab \n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej6Sabcd-1At"
      },
      "outputs": [],
      "source": [
        "# Read in data from S3 Bucket\n",
        "\n",
        "# Create a lookup table for the 500 cities. \n",
        "\n",
        "\n",
        "# Create a lookup table for the airport codes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0mhM9KxQkXS"
      },
      "outputs": [],
      "source": [
        "# Look over the delayed flight data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDfaP9NvQnh9"
      },
      "outputs": [],
      "source": [
        "# Look over the data of the 500 cities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LV1-sZ2QpZB"
      },
      "outputs": [],
      "source": [
        "# Look over the airport codes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_hKqNbW_EnA"
      },
      "outputs": [],
      "source": [
        "# Filter the airport codes to only contain rows whose `country` equals `USA`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vujvlsjxSj7N"
      },
      "outputs": [],
      "source": [
        "# Filter the latitude and longitude dataframe to only contain the 'name','latitude','longitude','admin1_code' fields and rows whose `country_code` equals `US`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DFhjm5Z_O5Z"
      },
      "outputs": [],
      "source": [
        "# Create temporary views for each of our DataFrames\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZdAUFJ3bDKQ"
      },
      "outputs": [],
      "source": [
        "# First, join the airport codes lookup table to the delayed flight DataFrame \n",
        "# and add the city of origin and destination like we did in the instructor demo.  \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eq0zGP4ciPp"
      },
      "outputs": [],
      "source": [
        "# Add the `origin_latitude` and `origin_longitude` fields by joining the `lookup_geo` view \n",
        "# to the `lookup_city` view and the delayed flight DataFrame.\n",
        "# Note:  The two lookup views do not have matching columns, so we must be mindful what names are used when joining both views together.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzMGI_BfXSc8"
      },
      "outputs": [],
      "source": [
        "# Finally, add the `dest_latitude` and `dest_longitude` fields by joining the `lookup_geo` view again as another alias, `geo_dest`.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgJRNc8h_bHI"
      },
      "outputs": [],
      "source": [
        "# Run the same query with a Broadcast hint for either table\n",
        " \n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "select /*+ BROADCAST() */ \n",
        "\n",
        "\"\"\").show()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhgb2iFihRbW"
      },
      "outputs": [],
      "source": [
        "# Write a sql (we used a CTE here) that does some aggregations on the new data.  The purpose of this SQL is to add some processing time.\n",
        "# Note the runtime\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "with allColumns\n",
        "(select /*+ BROADCAST() */ \n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPR_2uq0vdyt"
      },
      "outputs": [],
      "source": [
        "# Cache your largest temporary view\n",
        "# Note: when we use SparkSQL to cache a table, the table is immediately cached (no lazy evaluation), when using Pyspark it will not be cached until an action is ran.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eodqtIHByC2J"
      },
      "outputs": [],
      "source": [
        "# Check that your table is cached \n",
        "spark.catalog.isCached()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkSFfoBYwgSI"
      },
      "outputs": [],
      "source": [
        "# Run the same query again with the data cached. This should greatly improve the run time.  \n",
        "# Keep in mind we are not working with particularly large data here so the improvements may not be dramatic.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "with allColumns\n",
        "(select /*+ BROADCAST() */ \n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfvIwCCChuvc"
      },
      "outputs": [],
      "source": [
        "# you can even cache a large lookup table.\n",
        "spark.sql()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31NhqcGgh7HG"
      },
      "outputs": [],
      "source": [
        "# Run the same query again with the data cached. This should greatly improve the run time.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "with allColumns\n",
        "(select /*+ BROADCAST() */ \n",
        "\n",
        "\"\"\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P82CRVDxk7s"
      },
      "outputs": [],
      "source": [
        "# Remember to uncache the table as soon as you are done.\n",
        "spark.sql()\n",
        "spark.sql()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHEPdkk_x0Ep"
      },
      "outputs": [],
      "source": [
        "# Verify that the table is no longer cached\n",
        "if spark.catalog.isCached() or spark.catalog.isCached():\n",
        "  print(\"a table is till cached\")\n",
        "else:\n",
        "  print(\"all clear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XLLlG2kx4s3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cache_Broadcast_Unsolved.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
