{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZTcw6J61Azc"
      },
      "source": [
        "In this activity you will be importing data from github creating Spark dataframes.  Using the two data structures, you will create views that you can join and query to answer the question:\n",
        "What airport, city and state have the most departures?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2U0y6_MtZ9I",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example: 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnFy3YiptZkv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "# We are using pandas to read the raw csv files from github, then converting them to spark Dataframes (this will save us some download time and HDD space on our laptops)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
        "import pandas as pd\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDiqF0ortYYQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# URL to the RAW airport codes dataset.\n",
        "url='https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYmG-FkcuAnj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define the schema of the spark dataframe we intend to build from the csv.\n",
        "# *****Define Your Own Schema Here*****\n",
        "codesSchema= StructType(  \n",
        "                        \n",
        "                        )\n",
        "\n",
        "airportCodes=spark.createDataFrame(pd.read_csv(url, sep='\\t', error_bad_lines=False), schema=codesSchema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdP7_gxvuF5B",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Look at the airport code data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7WRSEVcvqCr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Read in the departures dataset\n",
        "urlDeparts='https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9IvUseLxYMP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# No need to define schema here but it would load much faster if you did.  Create a spark dataframe from the given url\n",
        "# Note the \"date\" field is actually MMDDHHmm, however if you read it without stating the dtype pandas will truncate leading 0\n",
        "# *****Define Your Own Schema Here*****\n",
        "depart_schema=StructType(\n",
        "                          \n",
        "                         )\n",
        "\n",
        "# We define the date (object) to retain the leading '0'\n",
        "airport_departs=spark.createDataFrame(pd.read_csv(urlDeparts , dtype={'date': object}), schema=depart_schema).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCcZcR2NxoVL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Look at the departure data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTnBJwtwx53G",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create a temporary view for your 'codes' dataframe\n",
        "#*****Your Code Begins Here*********\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctgkQWDyAT3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create a temporary view for your 'departures' dataframe\n",
        "#*****Your Code Begins Here*********\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRu69ik3yRL2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Write a sql statement that will return the columns origin, city, state and the number of departures in order from most to least\n",
        "#*****Your Code Begins Here*********\n",
        "sql_depByAir=\"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nZLW4pbyFOv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Execute the sql and save the results to a spark dataframe\n",
        "#*****Your Code Begins Here*********\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7aWImIYy6ef",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Show your results\n",
        "#*****Your Code Begins Here*********\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99zUcnYE4SRU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Airline_Departures_Unsolved.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
