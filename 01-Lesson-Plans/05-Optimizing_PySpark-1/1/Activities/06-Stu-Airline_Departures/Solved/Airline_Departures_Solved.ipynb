{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZTcw6J61Azc"
   },
   "source": [
    "In this activity you will be importing data from github creating Spark dataframes.  Using the two data structures, you will create views that you can join and query to answer the question:\n",
    "What airport, city and state have the most departures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2U0y6_MtZ9I",
    "outputId": "9ab4ce0e-f837-4135-8383-1c66aa82d295",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,006 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,297 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,861 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,040 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,294 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]\n",
      "Fetched 12.3 MB in 7s (1,707 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "# Activate Spark in our Colab notebook.\n",
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example: 'spark-3.2.2'\n",
    "spark_version = 'spark-3.2.2'\n",
    "# spark_version = 'spark-3.<enter version>'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnFy3YiptZkv",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# We are using pandas to read the raw csv files from github, then converting them to spark Dataframes (this will save us some download time and HDD space on our laptops)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDiqF0ortYYQ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# URL to the RAW airport codes dataset.\n",
    "url='https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYmG-FkcuAnj",
    "outputId": "165d4e70-5048-40d9-8093-ba02f52107ba",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Define the schema of the spark dataframe we intend to build from the csv.\n",
    "codesSchema= StructType(  \n",
    "                        [StructField(\"City\", StringType(), True), \n",
    "                         StructField(\"State\", StringType(), True),\n",
    "                         StructField(\"Country\", StringType(), True),\n",
    "                         StructField(\"Iata\", StringType(), True)]\n",
    "                        )\n",
    "\n",
    "airportCodes=spark.createDataFrame(pd.read_csv(url, sep='\\t', error_bad_lines=False), schema=codesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdP7_gxvuF5B",
    "outputId": "6e04c73f-af05-4eec-cd57-e24137b2d4ee",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(City='Abbotsford', State='BC', Country='Canada', Iata='YXX'),\n",
       " Row(City='Aberdeen', State='SD', Country='USA', Iata='ABR'),\n",
       " Row(City='Abilene', State='TX', Country='USA', Iata='ABI'),\n",
       " Row(City='Akron', State='OH', Country='USA', Iata='CAK'),\n",
       " Row(City='Alamosa', State='CO', Country='USA', Iata='ALS')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the airport code data\n",
    "airportCodes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7WRSEVcvqCr",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the departures dataset\n",
    "urlDeparts='https://raw.githubusercontent.com/databricks/LearningSparkV2/master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9IvUseLxYMP",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# No need to define schema here but it would load much faster if you did.  Create a spark dataframe from the given url\n",
    "# Note the \"date\" field is actually MMDDHHmm, however if you read it without stating the dtype pandas will truncate leading 0\n",
    "\n",
    "depart_schema=StructType(\n",
    "                          [StructField(\"date\", StringType(), True), \n",
    "                           StructField(\"delay\", IntegerType(), True),\n",
    "                           StructField(\"distance\", IntegerType(), True),\n",
    "                           StructField(\"origin\", StringType(), True),\n",
    "                           StructField(\"destination\", StringType(), True)]\n",
    "                         )\n",
    "\n",
    "#we define the date (object) to retain the leading '0'\n",
    "airport_departs=spark.createDataFrame(pd.read_csv(urlDeparts , dtype={'date': object}), schema=depart_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCcZcR2NxoVL",
    "outputId": "50f6f158-5e87-41a9-c687-f4cd5a6218b0",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='01011245', delay=6, distance=602, origin='ABE', destination='ATL'),\n",
       " Row(date='01020600', delay=-8, distance=369, origin='ABE', destination='DTW'),\n",
       " Row(date='01021245', delay=-2, distance=602, origin='ABE', destination='ATL'),\n",
       " Row(date='01020605', delay=-4, distance=602, origin='ABE', destination='ATL'),\n",
       " Row(date='01031245', delay=-4, distance=602, origin='ABE', destination='ATL')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the departure data\n",
    "airport_departs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTnBJwtwx53G",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for your 'codes' dataframe\n",
    "#*****Your Code Begins Here*********\n",
    "airportCodes.createOrReplaceTempView('codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OctgkQWDyAT3",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for your 'departures' dataframe\n",
    "#*****Your Code Begins Here*********\n",
    "airport_departs.createOrReplaceTempView('departures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRu69ik3yRL2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write a sql statement that will return the columns origin, city, state and the number of departures in order from most to least\n",
    "#*****Your Code Begins Here*********\n",
    "sql_depByAir=\"\"\"\n",
    "SELECT d.origin AS origin_Airport, c.city, c.State, count(*) AS origin_departures\n",
    "FROM departures d\n",
    "  JOIN\n",
    "    codes c\n",
    "    ON d.origin= c.Iata\n",
    "GROUP BY origin_Airport, c.city, c.State\n",
    "ORDER BY origin_departures DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nZLW4pbyFOv",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the sql and save the results to a spark dataframe\n",
    "#*****Your Code Begins Here*********\n",
    "df_departuresByAirport = spark.sql(sql_depByAir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7aWImIYy6ef",
    "outputId": "f2b38b16-db15-4bb0-9173-8c7ebdf82197",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----+-----------------+\n",
      "|origin_Airport|          city|State|origin_departures|\n",
      "+--------------+--------------+-----+-----------------+\n",
      "|           ATL|       Atlanta|   GA|            91484|\n",
      "|           DFW|        Dallas|   TX|            68482|\n",
      "|           ORD|       Chicago|   IL|            64228|\n",
      "|           LAX|   Los Angeles|   CA|            54086|\n",
      "|           DEN|        Denver|   CO|            53148|\n",
      "|           IAH|       Houston|   TX|            43361|\n",
      "|           PHX|       Phoenix|   AZ|            40155|\n",
      "|           SFO| San Francisco|   CA|            39483|\n",
      "|           LAS|     Las Vegas|   NV|            33107|\n",
      "|           CLT|     Charlotte|   NC|            28402|\n",
      "|           MCO|       Orlando|   FL|            28313|\n",
      "|           EWR|        Newark|   NJ|            27656|\n",
      "|           SLC|Salt Lake City|   UT|            25868|\n",
      "|           LGA|      New York|   NY|            25458|\n",
      "|           BOS|        Boston|   MA|            25348|\n",
      "|           MSP|   Minneapolis|   MN|            24031|\n",
      "|           JFK|      New York|   NY|            23572|\n",
      "|           DTW|       Detroit|   MI|            23421|\n",
      "|           SEA|       Seattle|   WA|            23078|\n",
      "|           MIA|         Miami|   FL|            21817|\n",
      "+--------------+--------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show your results\n",
    "#*****Your Code Begins Here*********\n",
    "df_departuresByAirport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99zUcnYE4SRU",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Airline_Departures_Solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
