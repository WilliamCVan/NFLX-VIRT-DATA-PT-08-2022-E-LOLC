{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crlKd6Qpq6BU",
        "outputId": "595b1bab-4f91-4c19-9eeb-5dfb9f4b5ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connected to cloud\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example: 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t3vOTJF5rD_4"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-mOHq6oaizia"
      },
      "outputs": [],
      "source": [
        "#After uploading this file to your colab environment, let's create a new spark dataframe\n",
        "df = spark.read.csv(\"/content/better_netflix_titles.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfSIDLbzn2IZ",
        "outputId": "45d954ba-f951-46eb-df71-8fa31a95f44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+-------+-----+-------------+-----------------+------------+------+---------+\n",
            "|_c0|show_id|   type|title|      country|       date_added|release_year|rating| duration|\n",
            "+---+-------+-------+-----+-------------+-----------------+------------+------+---------+\n",
            "|  0|     s1|TV Show|   3%|       Brazil|  August 14, 2020|        2020| TV-MA|4 Seasons|\n",
            "|  1|     s2|  Movie| 7:19|       Mexico|December 23, 2016|        2016| TV-MA|   93 min|\n",
            "|  2|     s3|  Movie|23:59|    Singapore|December 20, 2018|        2011|     R|   78 min|\n",
            "|  3|     s4|  Movie|    9|United States|November 16, 2017|        2009| PG-13|   80 min|\n",
            "|  4|     s5|  Movie|   21|United States|  January 1, 2020|        2008| PG-13|  123 min|\n",
            "|  5|     s6|TV Show|   46|       Turkey|     July 1, 2017|        2016| TV-MA| 1 Season|\n",
            "|  6|     s7|  Movie|  122|        Egypt|     June 1, 2020|        2019| TV-MA|   95 min|\n",
            "|  7|     s8|  Movie|  187|United States| November 1, 2019|        1997|     R|  119 min|\n",
            "|  8|     s9|  Movie|  706|        India|    April 1, 2019|        2019| TV-14|  118 min|\n",
            "|  9|    s10|  Movie| 1920|        India|December 15, 2017|        2008| TV-MA|  143 min|\n",
            "+---+-------+-------+-----+-------------+-----------------+------------+------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# take a look of what we have to work with\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MZLbpHe-n_MR"
      },
      "outputs": [],
      "source": [
        "# Create our temporary view\n",
        "df.createOrReplaceTempView('movies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednpJUL9oGTJ",
        "outputId": "1702c674-1a50-4590-e4e0-bb49239ff028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
            "|show_id| type|title|      country|date_added|release_year|rating|duration|\n",
            "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
            "|     s2|Movie| 7:19|       Mexico|2016-12-23|        2016| TV-MA|  93 min|\n",
            "|     s3|Movie|23:59|    Singapore|2018-12-20|        2011|     R|  78 min|\n",
            "|     s4|Movie|    9|United States|2017-11-16|        2009| PG-13|  80 min|\n",
            "|     s5|Movie|   21|United States|2020-01-01|        2008| PG-13| 123 min|\n",
            "|     s7|Movie|  122|        Egypt|2020-06-01|        2019| TV-MA|  95 min|\n",
            "|     s8|Movie|  187|United States|2019-11-01|        1997|     R| 119 min|\n",
            "|     s9|Movie|  706|        India|2019-04-01|        2019| TV-14| 118 min|\n",
            "|    s10|Movie| 1920|        India|2017-12-15|        2008| TV-MA| 143 min|\n",
            "|    s11|Movie| 1922|United States|2017-10-20|        2017| TV-MA| 103 min|\n",
            "|    s14|Movie|2,215|     Thailand|2019-03-01|        2018| TV-MA|  89 min|\n",
            "+-------+-----+-----+-------------+----------+------------+------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We can perform most any SQL action at this point\n",
        "# here we are converting the date to a more workable date object\n",
        "#NOTE: since we are not assigning this to a dataframe the change is not saved.\n",
        "spark.sql(\"\"\"SELECT show_id, \n",
        "   type, \n",
        "   title, \n",
        "   country, \n",
        "   TO_DATE(date_added, 'MMMM d, yyyy') \n",
        "   AS date_added, \n",
        "   release_year, \n",
        "   rating, \n",
        "   duration \n",
        "   FROM movies \n",
        "   WHERE date_added IS NOT null AND type='Movie'\"\"\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVT_qRomo959",
        "outputId": "2cedab4b-e695-4f71-cd16-b294708e007c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----------------+\n",
            "|  rating|number_of_ratings|\n",
            "+--------+-----------------+\n",
            "|   TV-MA|             2863|\n",
            "|   TV-14|             1931|\n",
            "|   TV-PG|              805|\n",
            "|       R|              665|\n",
            "|   PG-13|              386|\n",
            "|    TV-Y|              280|\n",
            "|   TV-Y7|              271|\n",
            "|      PG|              247|\n",
            "|    TV-G|              194|\n",
            "|      NR|               84|\n",
            "|       G|               39|\n",
            "|    null|                9|\n",
            "|TV-Y7-FV|                6|\n",
            "|      UR|                5|\n",
            "|   NC-17|                3|\n",
            "+--------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# All of the SQL you learned in Unit 6 is available to you in Spark SQL\n",
        "# Here we are listing out the counts by rating\n",
        "# NOTE: it is almost NEVER a good idea to \"order by\" when using Spark with large datasets (more on this in 8.2)\n",
        "spark.sql(\"\"\"\n",
        "  SELECT\n",
        "    rating,\n",
        "    count(*) AS number_of_ratings\n",
        "  FROM movies\n",
        "  GROUP BY rating\n",
        "  ORDER BY 2 DESC\n",
        "  \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1_PVtGyJzJqU"
      },
      "outputs": [],
      "source": [
        "# Let's output a file with just listing for children\n",
        "# first we will use our spark sql to write to a dataframe\n",
        "\n",
        "out_df= spark.sql(\"\"\"\n",
        "  SELECT \n",
        "  title,\n",
        "  rating,\n",
        "  date_added,\n",
        "  duration\n",
        "  FROM Movies\n",
        "  WHERE rating IN ('G','PG', 'PG-13')\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JoDAbSp0Eq0",
        "outputId": "31ab3acc-4e40-4077-c83c-36f0da70d967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+------------------+--------+\n",
            "|               title|rating|        date_added|duration|\n",
            "+--------------------+------+------------------+--------+\n",
            "|                   9| PG-13| November 16, 2017|  80 min|\n",
            "|                  21| PG-13|   January 1, 2020| 123 min|\n",
            "|            Ã†on Flux| PG-13|  February 1, 2018|  93 min|\n",
            "|         10,000 B.C.| PG-13|      June 1, 2019| 109 min|\n",
            "|           16 Blocks| PG-13|  November 1, 2019| 102 min|\n",
            "|            17 Again| PG-13|   January 1, 2021| 102 min|\n",
            "|20 Feet From Stardom| PG-13|September 22, 2018|  91 min|\n",
            "|             28 Days| PG-13|September 30, 2020| 104 min|\n",
            "|      3 Days to Kill| PG-13|  December 1, 2020| 117 min|\n",
            "|       3 Generations| PG-13|   August 28, 2017|  92 min|\n",
            "|            3 Idiots| PG-13|    August 1, 2019| 164 min|\n",
            "|        5 Flights Up| PG-13|    March 17, 2019|  92 min|\n",
            "|      50 First Dates| PG-13|  December 1, 2020|  99 min|\n",
            "|        A 2nd Chance|    PG|      July 1, 2017|  95 min|\n",
            "|     A Boy Called Po|    PG|  January 15, 2018|  94 min|\n",
            "|    A Bridge Too Far|    PG|      July 1, 2020| 176 min|\n",
            "|A California Chri...| PG-13| December 14, 2020| 107 min|\n",
            "|    A Champion Heart|     G|    April 14, 2020|  90 min|\n",
            "|  A Cinderella Story|    PG|   January 1, 2020|  95 min|\n",
            "|A Cinderella Stor...|    PG|  December 1, 2019|  86 min|\n",
            "+--------------------+------+------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# make sure we got what we wanted\n",
        "out_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z4csQ38M6xCl"
      },
      "outputs": [],
      "source": [
        "#  As Spark stores the data in partitions, it will also write data in partitions.\n",
        "#  These partitions will always be stored in a folder with the same name as the file, and that folder may often contain many subfolders or files.\n",
        "#  Within the partition folder, there will be a file or files that starts with `part-`, these are CSV files. \n",
        "# However, they are often not optimal for friendly reading, but can be downloaded to your computer.\n",
        "\n",
        "out_df.write.csv('movies_out_spark.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wAbO6VJL65Nb"
      },
      "outputs": [],
      "source": [
        "# The easiest work around of the part file output is to take the data to Pandas and write out a CSV.\n",
        "# This forces the data to the master node and is not recommended unless you have filtered and/or aggregated your data to a reasonable size.\n",
        "\n",
        "out_df.toPandas().to_csv('movies_out_pandas.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn9uNP137FdI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "SparkSQL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
