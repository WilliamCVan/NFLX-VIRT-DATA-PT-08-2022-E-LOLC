{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_KW73O2e3dw"
      },
      "outputs": [],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example: 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXHHGYjWzJBg"
      },
      "outputs": [],
      "source": [
        "# Install great expectations\n",
        "!pip install great_expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession and great expectations \n",
        "from pyspark.sql import SparkSession\n",
        "import great_expectations as ge\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "# Start Spark session\n",
        "from pyspark import SparkFiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiVGvvzwgGXe"
      },
      "outputs": [],
      "source": [
        "# Read the data into a Spark DataFrame.\n",
        "employees_df = spark.read.csv('/content/attrition.csv', sep=',', header=True)\n",
        "employees_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7qrX9vZ0tUF"
      },
      "outputs": [],
      "source": [
        "# Save the column names to a list. \n",
        "columns = employees_df.schema.names\n",
        "print(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ofXYDMUHN6V"
      },
      "outputs": [],
      "source": [
        "# Create the Great Expectations DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JyGKtiBzW_-"
      },
      "outputs": [],
      "source": [
        "# Test that each column doesn't contain null values. If a column does contain null values print out the column name.\n",
        "results = []\n",
        "for column in columns:\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrBMWx9ynnOi"
      },
      "outputs": [],
      "source": [
        "# Use a function to remove the null values and bad data from a temporary view and save the data with null values to a parquet file. \n",
        "def clean_data():\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSdn_ycvN6IW"
      },
      "outputs": [],
      "source": [
        "# Retrieve the \"success\" fields for the columns that meet the criteria in the clean_data() function.\n",
        "\n",
        "# If the columns meet the criteria call the clean_data() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cnIQcvCL_7_"
      },
      "outputs": [],
      "source": [
        "# Read in our customers parquet data\n",
        "\n",
        "# Get the number of rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28RIwfL-P8yj"
      },
      "outputs": [],
      "source": [
        "# Use a function to transform the employees temporary view if the id is not in the removed_employees.\n",
        "def transform_data():\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s89jTmD2O16n"
      },
      "outputs": [],
      "source": [
        "# Call the transformation function and convert the transformed DataFrame into a Great Expectations DataFrame\n",
        "\n",
        "# Retrieve the \"success\" fields for the transformed DataFrame.\n",
        "\n",
        "# If the \"success\" field is 'False' print \"Failed\", if not then write the transformed DataFrame to a parquet file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlNrERvzOrSG"
      },
      "outputs": [],
      "source": [
        "# Read in our customers parquet data\n",
        "\n",
        "# Get the number of rows. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GE_attrition_unsolved.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('qatest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "df07ec09c4017cff5cffe5c6e95982b3fd889b25c9af2c66175cc1127f95fbfd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
